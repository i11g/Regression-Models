import numpy as np
import matplotlib.pyplot as plt
import pandas as pd
import seaborn as sns
import os
from time import perf_counter
from sklearn.datasets import make_blobs














# Set dimensions
n, p = 200, 10   # 200 observations, 10 features

# Generate features with different distributions/ranges
np.random.seed(42)  # reproducibility
X = np.column_stack([
    np.random.uniform(0, 10, n),         # Uniform(0,10)
    np.random.normal(0, 1, n),           # Normal(0,1)
    np.random.exponential(1, n),         # Exponential
    np.random.randint(0, 100, n),        # Random integers
    np.random.uniform(50, 100, n),       # Uniform(50,100)
    np.random.normal(10, 5, n),          # Normal(mean=10, std=5)
    np.random.chisquare(df=2, size=n),   # Chi-square
    np.random.beta(2, 5, n) * 20,        # Beta scaled
    np.random.binomial(10, 0.3, n),      # Binomial
    np.random.poisson(5, n)              # Poisson
])

# True coefficients (hidden later)
beta_true = np.random.uniform(1, 5, p)  # coefficients between 1 and 5

# Generate y with noise
noise = np.random.normal(0, 5, n)  # Gaussian noise
y = X @ beta_true + noise

# Save dataset
df = pd.DataFrame(X, columns=[f"x{i+1}" for i in range(p)])
df["y"] = y
df.to_csv("linear_regression_dataset.csv", index=False)

print("Dataset generated and saved as 'linear_regression_dataset.csv'")
print(df.head())








# Load dataset
df = pd.read_csv("linear_regression_dataset.csv")
print(df.head())

# Basic statistics
print("\n=== Summary Statistics ===")
print(df.describe().round(2))

# Correlation with target
corr = df.corr(numeric_only=True)
print("\n=== Correlation with y ===")
print(corr["y"].sort_values(ascending=False))

# Heatmap of correlations
plt.figure(figsize=(10, 6))
sns.heatmap(corr, annot=False, cmap="coolwarm", center=0)
plt.title("Feature Correlations with Target")
plt.show()

# Scatterplots for first few features
sns.pairplot(df, x_vars=["x1", "x2", "x3", "x4", "x5"], y_vars="y", height=3)
plt.show()














def linear_model(X, beta):
    """
    Compute predictions y = X' @ beta
    where X is (n, p), beta is (p+1,)
    
    X: feature matrix of shape (n, p)
    beta: coefficient vector including intercept (length p+1)
    """
    # Add column of ones for intercept
    X_aug = np.column_stack([np.ones(X.shape[0]), X])
    return X_aug @ beta

# --- Test the function ---
# Small test dataset
X_test = np.array([[1, 2],
                   [3, 4],
                   [5, 6]])  # shape (3,2)

beta_test = np.array([10, 2, 3]) 
# This means: y = 10 + 2*x1 + 3*x2

y_pred = linear_model(X_test, beta_test)
print("Test predictions:", y_pred)








def augment_X_with_ones(X: np.ndarray) -> np.ndarray:
    """Add intercept column: X' = [1, X]."""
    return np.column_stack([np.ones(X.shape[0]), X])

def mse_cost(X: np.ndarray, beta: np.ndarray, y: np.ndarray) -> float:
    """
    Mean Squared Error:
    J(beta) = (1/n) * || y - X' @ beta ||^2
    X: (n, p), beta: (p+1,), y: (n,)
    """
    X_aug = augment_X_with_ones(X)
    residuals = X_aug @ beta - y
    n = X_aug.shape[0]
    return (residuals @ residuals) / n

def mse_grad(X: np.ndarray, beta: np.ndarray, y: np.ndarray) -> np.ndarray:
    """
    Gradient of MSE w.r.t. beta:
    grad = (2/n) * X'^T * (X' @ beta - y)
    Returns shape (p+1,)
    """
    X_aug = augment_X_with_ones(X)
    residuals = X_aug @ beta - y
    n = X_aug.shape[0]
    return (2.0 / n) * (X_aug.T @ residuals)


# Synthetic small test
np.random.seed(0)
n, p = 5, 3
X_test = np.random.randn(n, p)
beta_test = np.random.randn(p + 1)
y_test = np.random.randn(n)

# Analytical gradient
g_analytical = mse_grad(X_test, beta_test, y_test)

# Finite-difference gradient
eps = 1e-6
g_numeric = np.zeros_like(beta_test)
for j in range(len(beta_test)):
    e = np.zeros_like(beta_test); e[j] = eps
    j_plus  = mse_cost(X_test, beta_test + e, y_test)
    j_minus = mse_cost(X_test, beta_test - e, y_test)
    g_numeric[j] = (j_plus - j_minus) / (2 * eps)

print("Analytical grad:", np.round(g_analytical, 6))
print("Numeric grad   :", np.round(g_numeric, 6))
print("Max |diff|     :", float(np.max(np.abs(g_analytical - g_numeric))))









# --- Reuse from Task 4 ---
def augment_X_with_ones(X: np.ndarray) -> np.ndarray:
    return np.column_stack([np.ones(X.shape[0]), X])

def mse_cost(X: np.ndarray, beta: np.ndarray, y: np.ndarray) -> float:
    X_aug = augment_X_with_ones(X)
    r = X_aug @ beta - y
    return (r @ r) / X.shape[0]

def mse_grad(X: np.ndarray, beta: np.ndarray, y: np.ndarray) -> np.ndarray:
    X_aug = augment_X_with_ones(X)
    r = X_aug @ beta - y
    n = X.shape[0]
    return (2.0 / n) * (X_aug.T @ r)

# --- Utilities for scaling and safe step size ---
class StandardScaler:
    def fit(self, X):
        self.mean_ = X.mean(axis=0)
        self.std_  = X.std(axis=0, ddof=0)
        self.std_[self.std_ == 0.0] = 1.0
        return self
    def transform(self, X):
        return (X - self.mean_) / self.std_
    def fit_transform(self, X):
        return self.fit(X).transform(X)

def safe_eta_from_lipschitz(X: np.ndarray) -> float:
    """
    Compute a safe learning rate via L = (2/n) * lambda_max(X'^T X')
    where X' = [1, X]. Then eta = 1/L.
     """
    X_aug = augment_X_with_ones(X)
    # Compute the largest eigenvalue of X_aug^T X_aug
    # For numerical stability use svd: lambda_max = sigma_max^2
    # where sigma_max is the largest singular value of X_aug.
    smax = np.linalg.svd(X_aug, compute_uv=False)[0]
    n = X.shape[0]
    L = (2.0 / n) * (smax ** 2)
    return 1.0 / (L + 1e-12)

def gradient_descent(
    X: np.ndarray,
    y: np.ndarray,
    eta: float | None = None,
    max_iters: int = 10000,
    tol: float = 1e-8,
    grad_tol: float = 1e-6,
    patience: int = 100,
    normalize: bool = True,
    seed: int = 0,
):
    """
    Perform GD on MSE. Returns beta, history, and metadata.
    - Stopping rules: max_iters, relative cost improvement, gradient norm, patience.
    - If eta is None, use safe Lipschitz-based step size.
    """
    rng = np.random.default_rng(seed)

    # Optionally standardize X (intercept handled by augment function)
    scaler = None
    X_work = X.copy()
    if normalize:
        scaler = StandardScaler()
        X_work = scaler.fit_transform(X_work)

    # Initialize beta: zeros is fine with normalized features
    beta = np.zeros(X_work.shape[1] + 1)

    # Learning rate
    if eta is None:
        eta = safe_eta_from_lipschitz(X_work)

    history = {"cost": [], "grad_norm": [], "eta": eta}
    best_cost = np.inf
    no_improve = 0

    prev_cost = None
    for t in range(1, max_iters + 1):
        g = mse_grad(X_work, beta, y)
        beta -= eta * g

        J = mse_cost(X_work, beta, y)
        gnorm = np.linalg.norm(g)

        history["cost"].append(J)
        history["grad_norm"].append(gnorm)

        # Stopping: gradient small
        if gnorm < grad_tol:
            return beta, history, {"iters": t, "converged": True, "reason": "grad_tol", "eta": eta}

        # Stopping: relative improvement small (after first step)
        if prev_cost is not None:
            rel_drop = (prev_cost - J) / max(prev_cost, 1.0)
            if rel_drop < tol:
                no_improve += 1
            else:
                no_improve = 0

            if no_improve >= patience:
                return beta, history, {"iters": t, "converged": True, "reason": "patience", "eta": eta}

        # Track best (optional)
        if J < best_cost:
            best_cost = J

        prev_cost = J

    return beta, history, {"iters": max_iters, "converged": False, "reason": "max_iters", "eta": eta}



# --- Example run on our CSV dataset ---
df = pd.read_csv("linear_regression_dataset.csv")
X = df.drop(columns=["y"]).to_numpy()
y = df["y"].to_numpy()

beta_gd, hist, info = gradient_descent(
    X, y,
    eta=None,            # auto-compute safe step
    max_iters=20000,
    tol=1e-9,
    grad_tol=1e-6,
    patience=200,
    normalize=True,
)

print("Converged:", info["converged"], "| Reason:", info["reason"], "| Iters:", info["iters"])
print("Step size (eta):", hist["eta"])
print("Final cost:", hist["cost"][-1])
print("Final grad norm:", hist["grad_norm"][-1])
print("Estimated beta (first 6):", np.round(beta_gd[:6], 4))


import matplotlib.pyplot as plt

plt.figure(figsize=(6,4))
plt.plot(hist["cost"])
plt.xlabel("Iteration")
plt.ylabel("MSE cost")
plt.title("Gradient Descent Learning Curve")
plt.grid(True, linewidth=0.3)
plt.show()









# --- Common helpers (reuse from earlier tasks) ---
def augment_X_with_ones(X): return np.column_stack([np.ones(X.shape[0]), X])

class StandardScaler:
    def fit(self, X):
        self.mean_ = X.mean(axis=0)
        self.std_  = X.std(axis=0, ddof=0)
        self.std_[self.std_ == 0.0] = 1.0
        return self
    def transform(self, X): return (X - self.mean_) / self.std_
    def fit_transform(self, X): return self.fit(X).transform(X)

def safe_eta_from_lipschitz(X):
    """Safe step using L = (2/n) * sigma_max(X')^2 (like MSE upper bound).
    We'll scale down for MAE/Huber to be conservative."""
    X_aug = augment_X_with_ones(X)
    smax = np.linalg.svd(X_aug, compute_uv=False)[0]
    n = X.shape[0]
    L = (2.0 / n) * (smax**2)
    return 1.0 / (L + 1e-12)

# --- Losses and (sub)gradients ---
def cost_mae(X, beta, y):
    X_aug = augment_X_with_ones(X)
    r = X_aug @ beta - y
    return np.mean(np.abs(r))

def grad_mae(X, beta, y):
    X_aug = augment_X_with_ones(X)
    r = X_aug @ beta - y
    s = np.sign(r)            # subgradient; sign(0)=0
    return (X_aug.T @ s) / X.shape[0]

def cost_huber(X, beta, y, delta=1.0):
    X_aug = augment_X_with_ones(X)
    r = X_aug @ beta - y
    a = np.abs(r)
    quad = 0.5 * (r**2)
    lin  = delta * (a - 0.5 * delta)
    return np.mean(np.where(a <= delta, quad, lin))

def grad_huber(X, beta, y, delta=1.0):
    X_aug = augment_X_with_ones(X)
    r = X_aug @ beta - y
    a = np.abs(r)
    psi = np.where(a <= delta, r, delta * np.sign(r))
    return (X_aug.T @ psi) / X.shape[0]

# --- Generic GD driver (works for MAE/Huber, also MSE if you pass its cost/grad) ---
def gradient_descent_generic(
    X, y, cost_fn, grad_fn, *,
    eta=None, max_iters=20000, tol=1e-8, grad_tol=1e-6, patience=200,
    normalize=True, seed=0, grad_kwargs=None, cost_kwargs=None, eta_scale=1.0
):
    rng = np.random.default_rng(seed)
    grad_kwargs = grad_kwargs or {}
    cost_kwargs = cost_kwargs or {}

    # Optional standardization (recommended for stability)
    scaler = None
    X_work = X.copy()
    if normalize:
        scaler = StandardScaler()
        X_work = scaler.fit_transform(X_work)

    # Init
    beta = np.zeros(X_work.shape[1] + 1)

    # Learning rate
    if eta is None:
        eta = safe_eta_from_lipschitz(X_work) * eta_scale  # often need smaller steps vs MSE

    hist = {"cost": [], "grad_norm": [], "eta": eta}
    best_cost = np.inf; no_improve = 0; prev_cost = None
    t0 = perf_counter()
    for t in range(1, max_iters + 1):
        g = grad_fn(X_work, beta, y, **grad_kwargs)
        beta -= eta * g
        J = cost_fn(X_work, beta, y, **cost_kwargs)
        gnorm = np.linalg.norm(g)
        hist["cost"].append(J); hist["grad_norm"].append(gnorm)

        # Stopping: gradient small (works well for Huber; for MAE this is a subgradient)
        if gnorm < grad_tol:
            dt = perf_counter() - t0
            return beta, hist, {"iters": t, "converged": True, "reason": "grad_tol", "eta": eta, "time_s": dt}

        # Stopping: relative cost improvement with patience
        if prev_cost is not None:
            rel_drop = (prev_cost - J) / max(prev_cost, 1.0)
            if rel_drop < tol: no_improve += 1
            else: no_improve = 0
            if no_improve >= patience:
                dt = perf_counter() - t0
                return beta, hist, {"iters": t, "converged": True, "reason": "patience",
                                    "eta": eta, "time_s": dt}
        prev_cost = J

    dt = perf_counter() - t0
    return beta, hist, {"iters": max_iters, "converged": False, "reason": "max_iters", 
                        "eta": eta, "time_s": dt}


# Load your data
df = pd.read_csv("linear_regression_dataset.csv")
X = df.drop(columns=["y"]).to_numpy()
y = df["y"].to_numpy()

# (Optional) Inject outliers to test robustness:
# out_idx = np.random.default_rng(123).choice(len(y), size=5, replace=False)
# y[out_idx] += 800  # large positive outliers

# MAE: subgradient descent — smaller eta, looser grad_tol typically helps
beta_mae, hist_mae, info_mae = gradient_descent_generic(
    X, y, cost_mae, grad_mae,
    eta=None, eta_scale=0.25,     # shrink LR vs MSE heuristic
    max_iters=50000, tol=1e-9, grad_tol=5e-5, patience=1000,
    normalize=True
)

# Huber: choose delta relative to noise scale; try delta ≈ 1–2× RMSE(MSE) or use a robust scale
delta = 5.0
beta_hub, hist_hub, info_hub = gradient_descent_generic(
    X, y,
    lambda X_, b, y_: cost_huber(X_, b, y_, delta=delta),
    lambda X_, b, y_: grad_huber(X_, b, y_, delta=delta),
    eta=None, eta_scale=0.5,      # Huber tolerates larger steps than MAE
    max_iters=20000, tol=1e-9, grad_tol=1e-6, patience=500,
    normalize=True
)

print("MAE   -> Converged:", info_mae["converged"], "| Reason:", info_mae["reason"],
      "| Iters:", info_mae["iters"], "| Time (s):", round(info_mae["time_s"], 4),
      "| Final cost:", round(hist_mae["cost"][-1], 6))

print("Huber -> Converged:", info_hub["converged"], "| Reason:", info_hub["reason"],
      "| Iters:", info_hub["iters"], "| Time (s):", round(info_hub["time_s"], 4),
      "| Final cost:", round(hist_hub["cost"][-1], 6), "| delta:", delta)









# ---------- Reuse / minimal defs ----------
def augment_X_with_ones(X): return np.column_stack([np.ones(X.shape[0]), X])

class StandardScaler:
    def fit(self, X):
        self.mean_ = X.mean(axis=0)
        self.std_  = X.std(axis=0, ddof=0)
        self.std_[self.std_ == 0.0] = 1.0
        return self
    def transform(self, X): return (X - self.mean_) / self.std_
    def fit_transform(self, X): return self.fit(X).transform(X)

def safe_eta_from_lipschitz(X):
    X_aug = augment_X_with_ones(X)
    smax = np.linalg.svd(X_aug, compute_uv=False)[0]  # largest singular value
    n = X.shape[0]
    L = (2.0 / n) * (smax**2)
    return 1.0 / (L + 1e-12)

# Losses
def cost_huber(X, beta, y, delta=5.0):
    X_aug = augment_X_with_ones(X)
    r = X_aug @ beta - y
    a = np.abs(r)
    quad = 0.5 * (r**2)
    lin  = delta * (a - 0.5 * delta)
    return np.mean(np.where(a <= delta, quad, lin))

def grad_huber(X, beta, y, delta=5.0):
    X_aug = augment_X_with_ones(X)
    r = X_aug @ beta - y
    a = np.abs(r)
    psi = np.where(a <= delta, r, delta * np.sign(r))
    return (X_aug.T @ psi) / X.shape[0]

# (For evaluation on same scale)
def predict_from_std_beta(X, beta_std, mu, sigma):
    b0, w = beta_std[0], beta_std[1:]
    X_std = (X - mu) / sigma
    return b0 + X_std @ w

def eval_all(y_true, y_pred, delta=5.0):
    r = y_pred - y_true
    mse = float(np.mean(r**2))
    mae = float(np.mean(np.abs(r)))
    hub = float(np.mean(np.where(np.abs(r)<=delta, 0.5*r**2, delta*(np.abs(r)-0.5*delta))))
    return mse, mae, hub

# Generic GD for a given fixed eta (no auto-LR)
def gd_fixed_eta(
    X, y, *, eta, cost_fn, grad_fn, cost_kwargs=None, grad_kwargs=None,
    max_iters=20000, tol=1e-9, grad_tol=1e-6, patience=500, normalize=True, seed=0
):
    cost_kwargs = cost_kwargs or {}
    grad_kwargs = grad_kwargs or {}

    # Scale features
    Xw = X.copy()
    scaler = None
    if normalize:
        scaler = StandardScaler()
        Xw = scaler.fit_transform(Xw)

    beta = np.zeros(Xw.shape[1] + 1)
    hist_cost, hist_gn = [], []
    prev_cost, no_imp = None, 0

    t0 = perf_counter()
    for t in range(1, max_iters + 1):
        g = grad_fn(Xw, beta, y, **grad_kwargs)
        beta -= eta * g
        J = cost_fn(Xw, beta, y, **cost_kwargs)
        gn = float(np.linalg.norm(g))

        hist_cost.append(float(J))
        hist_gn.append(gn)

        # NaN/Inf guard (divergence)
        if not np.isfinite(J) or not np.isfinite(gn):
            dt = perf_counter() - t0
            return beta, {"converged": False, "reason": "nan/inf", "iters": t, "time_s": dt,
                          "final_cost": float(J), "final_grad_norm": gn, "hist_cost": hist_cost}

        if gn < grad_tol:
            dt = perf_counter() - t0
            return beta, {"converged": True, "reason": "grad_tol", "iters": t, "time_s": dt,
                          "final_cost": float(J), "final_grad_norm": gn, "hist_cost": hist_cost}

        if prev_cost is not None:
            rel_drop = (prev_cost - J) / max(prev_cost, 1.0)
            if rel_drop < tol: no_imp += 1
            else: no_imp = 0
            if no_imp >= patience:
                dt = perf_counter() - t0
                return beta, {"converged": True, "reason": "patience", "iters": t, "time_s": dt,
                              "final_cost": float(J), "final_grad_norm": gn, "hist_cost": hist_cost}
        prev_cost = J

    dt = perf_counter() - t0
    return beta, {"converged": False, "reason": "max_iters", "iters": max_iters, "time_s": dt,
                  "final_cost": float(hist_cost[-1]), "final_grad_norm": float(hist_gn[-1]), "hist_cost": hist_cost}

# ---------- Run experiments ----------
df = pd.read_csv("linear_regression_dataset.csv")
X = df.drop(columns=["y"]).to_numpy()
y = df["y"].to_numpy()

# Choose Huber as favorite; set delta near residual scale (we used ~5 earlier)
delta = 5.0
cost_fn = lambda X_, b, y_: cost_huber(X_, b, y_, delta=delta)
grad_fn = lambda X_, b, y_: grad_huber(X_, b, y_, delta=delta)

# Build a wide range of learning rates around the "safe" one
eta_safe = safe_eta_from_lipschitz((X - X.mean(0)) / X.std(0, ddof=0))
etas = [
    eta_safe * f for f in
    [0.01, 0.05, 0.1, 0.2, 0.5, 1.0, 2.0, 5.0, 10.0, 20.0]
]

results = []
mu = X.mean(axis=0); sigma = X.std(axis=0, ddof=0); sigma[sigma==0]=1.0

for eta in etas:
    beta, info = gd_fixed_eta(
        X, y, eta=eta, cost_fn=cost_fn, grad_fn=grad_fn,
        cost_kwargs={}, grad_kwargs={}, max_iters=20000,
        tol=1e-9, grad_tol=1e-6, patience=1000, normalize=True
    )
    # Evaluate in original space on common metrics
    y_pred = predict_from_std_beta(X, beta, mu, sigma)
    mse, mae, hub = eval_all(y, y_pred, delta=delta)

    results.append({
        "eta": eta,
        "converged": info["converged"],
        "reason": info["reason"],
        "iters": info["iters"],
        "time_s": round(info["time_s"], 4),
        "final_cost(Huber)": round(info["final_cost"], 6),
        "final_grad_norm": f'{info["final_grad_norm"]:.2e}',
        "eval_MSE": round(mse, 6),
        "eval_MAE": round(mae, 6),
        f"eval_Huber(δ={delta})": round(hub, 6),
    })

pd.DataFrame(results)











# Number of samples and features (same as regression problem)
n, p = 200, 10

# Generate 2 clusters
X_class, y_class = make_blobs(
    n_samples=n,
    centers=2,           # two classes
    n_features=p,        # same as regression
    cluster_std=3.0,     # spread of the clusters
    random_state=42
)

# Wrap into DataFrame for convenience
df_class = pd.DataFrame(X_class, columns=[f"x{i+1}" for i in range(p)])
df_class["label"] = y_class

print(df_class.head())


plt.figure(figsize=(6,6))
plt.scatter(X_class[:,0], X_class[:,1], c=y_class, cmap="bwr", alpha=0.7)
plt.xlabel("x1")
plt.ylabel("x2")
plt.title("Two Clusters (first two features)")
plt.show()








# --- Reuse helpers ---
def augment_X_with_ones(X): return np.column_stack([np.ones(X.shape[0]), X])

class StandardScaler:
    def fit(self, X):
        self.mean_ = X.mean(axis=0)
        self.std_  = X.std(axis=0, ddof=0)
        self.std_[self.std_ == 0.0] = 1.0
        return self
    def transform(self, X): return (X - self.mean_) / self.std_
    def fit_transform(self, X): return self.fit(X).transform(X)

def safe_eta_from_lipschitz_logistic(X):
    """
    Logistic loss is 1/4-Lipschitz in the Hessian along X' for probabilities in (0,1).
    A conservative bound: L <= (1/4n) * lambda_max(X'^T X').
    We'll use eta = 1 / L as a 'safe' starting LR.
    """
    X_aug = augment_X_with_ones(X)
    smax = np.linalg.svd(X_aug, compute_uv=False)[0]  # largest singular value
    n = X.shape[0]
    L = (0.25 / n) * (smax**2)
    return 1.0 / (L + 1e-12)

# --- Logistic model ---
def sigmoid(z): return 1.0 / (1.0 + np.exp(-z))

def logistic_model(X, beta):
    X_aug = augment_X_with_ones(X)
    return sigmoid(X_aug @ beta)  # probabilities for class 1

def logistic_cost(X, beta, y):
    y_hat = logistic_model(X, beta)
    eps = 1e-12
    y_hat = np.clip(y_hat, eps, 1 - eps)
    return -np.mean(y * np.log(y_hat) + (1 - y) * np.log(1 - y_hat))

def logistic_grad(X, beta, y):
    X_aug = augment_X_with_ones(X)
    y_hat = sigmoid(X_aug @ beta)
    return (X_aug.T @ (y_hat - y)) / X.shape[0]





def gd_fixed_eta(
    X, y, *, eta, cost_fn, grad_fn,
    max_iters=20000, tol=1e-9, grad_tol=1e-6, patience=1000,
    normalize=True, seed=0
):
    # Optional standardization (helps conditioning)
    Xw = X.copy()
    scaler = None
    if normalize:
        scaler = StandardScaler()
        Xw = scaler.fit_transform(Xw)

    rng = np.random.default_rng(seed)
    beta = np.zeros(Xw.shape[1] + 1)

    hist_cost, hist_gn = [], []
    prev_cost, no_imp = None, 0

    t0 = perf_counter()
    for t in range(1, max_iters + 1):
        g = grad_fn(Xw, beta, y)
        beta -= eta * g

        J = cost_fn(Xw, beta, y)
        gn = float(np.linalg.norm(g))
        hist_cost.append(float(J))
        hist_gn.append(gn)

        if gn < grad_tol:
            dt = perf_counter() - t0
            return beta, {"converged": True, "reason": "grad_tol", "iters": t, "time_s": dt,
                          "final_cost": float(J), "final_grad_norm": gn, "hist_cost": hist_cost}

        if prev_cost is not None:
            rel_drop = (prev_cost - J) / max(prev_cost, 1.0)
            if rel_drop < tol: no_imp += 1
            else: no_imp = 0
            if no_imp >= patience:
                dt = perf_counter() - t0
                return beta, {"converged": True, "reason": "patience", "iters": t, "time_s": dt,
                              "final_cost": float(J), "final_grad_norm": gn, "hist_cost": hist_cost}
        prev_cost = J

    dt = perf_counter() - t0
    return beta, {"converged": False, "reason": "max_iters", "iters": max_iters, "time_s": dt,
                  "final_cost": float(hist_cost[-1]), "final_grad_norm": float(hist_gn[-1]),
                  "hist_cost": hist_cost}






# Use the clusters you generated earlier:
# df_class with columns x1..x10 and 'label'
Xc = df_class.drop(columns=["label"]).to_numpy()
yc = df_class["label"].to_numpy().astype(float)

# Build LR grid around a 'safe' logistic LR
eta_safe = safe_eta_from_lipschitz_logistic((Xc - Xc.mean(0)) / Xc.std(0, ddof=0))
etas = [eta_safe * f for f in [0.01, 0.05, 0.1, 0.2, 0.5, 1.0, 2.0, 5.0, 10.0]]

def predict_class(X, beta, thresh=0.5):
    p = logistic_model((X - X.mean(0)) / X.std(0, ddof=0), beta)  # use same normalization convention
    return (p >= thresh).astype(int), p

def metrics_classification(y_true, y_prob, threshold=0.5):
    y_pred = (y_prob >= threshold).astype(int)
    acc = float(np.mean(y_pred == y_true))
    # log-loss for reporting (same as cost but on raw X for clarity)
    eps = 1e-12
    y_prob_c = np.clip(y_prob, eps, 1-eps)
    logloss = -np.mean(y_true * np.log(y_prob_c) + (1-y_true) * np.log(1-y_prob_c))
    return acc, logloss

rows = []
for eta in etas:
    beta, info = gd_fixed_eta(
        Xc, yc, eta=eta,
        cost_fn=logistic_cost, grad_fn=logistic_grad,
        max_iters=20000, tol=1e-9, grad_tol=1e-6, patience=2000,
        normalize=True
    )
    # Evaluate on the same data (train metrics)
    # Use the same normalization as training when calling model; reuse helper:
    # For simplicity, rebuild the scaler here consistent with gd_fixed_eta:
    mu = Xc.mean(axis=0); sd = Xc.std(axis=0, ddof=0); sd[sd==0]=1.0
    Xc_std = (Xc - mu) / sd
    y_prob = logistic_model(Xc_std, beta)
    acc, logloss = metrics_classification(yc, y_prob, threshold=0.5)

    rows.append({
        "eta": eta,
        "converged": info["converged"],
        "reason": info["reason"],
        "iters": info["iters"],
        "time_s": round(info["time_s"], 4),
        "final_cost": round(info["final_cost"], 6),
        "final_grad_norm": f'{info["final_grad_norm"]:.2e}',
        "train_accuracy": round(acc, 4),
        "train_logloss": round(logloss, 6),
    })

pd.DataFrame(rows)









