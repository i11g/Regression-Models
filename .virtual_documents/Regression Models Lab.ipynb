import numpy as np
import matplotlib.pyplot as plt
import pandas as pd
import seaborn as sns
import os
from time import perf_counter














# Set dimensions
n, p = 200, 10   # 200 observations, 10 features

# Generate features with different distributions/ranges
np.random.seed(42)  # reproducibility
X = np.column_stack([
    np.random.uniform(0, 10, n),         # Uniform(0,10)
    np.random.normal(0, 1, n),           # Normal(0,1)
    np.random.exponential(1, n),         # Exponential
    np.random.randint(0, 100, n),        # Random integers
    np.random.uniform(50, 100, n),       # Uniform(50,100)
    np.random.normal(10, 5, n),          # Normal(mean=10, std=5)
    np.random.chisquare(df=2, size=n),   # Chi-square
    np.random.beta(2, 5, n) * 20,        # Beta scaled
    np.random.binomial(10, 0.3, n),      # Binomial
    np.random.poisson(5, n)              # Poisson
])

# True coefficients (hidden later)
beta_true = np.random.uniform(1, 5, p)  # coefficients between 1 and 5

# Generate y with noise
noise = np.random.normal(0, 5, n)  # Gaussian noise
y = X @ beta_true + noise

# Save dataset
df = pd.DataFrame(X, columns=[f"x{i+1}" for i in range(p)])
df["y"] = y
df.to_csv("linear_regression_dataset.csv", index=False)

print("Dataset generated and saved as 'linear_regression_dataset.csv'")
print(df.head())








# Load dataset
df = pd.read_csv("linear_regression_dataset.csv")
print(df.head())

# Basic statistics
print("\n=== Summary Statistics ===")
print(df.describe().round(2))

# Correlation with target
corr = df.corr(numeric_only=True)
print("\n=== Correlation with y ===")
print(corr["y"].sort_values(ascending=False))

# Heatmap of correlations
plt.figure(figsize=(10, 6))
sns.heatmap(corr, annot=False, cmap="coolwarm", center=0)
plt.title("Feature Correlations with Target")
plt.show()

# Scatterplots for first few features
sns.pairplot(df, x_vars=["x1", "x2", "x3", "x4", "x5"], y_vars="y", height=3)
plt.show()














def linear_model(X, beta):
    """
    Compute predictions y = X' @ beta
    where X is (n, p), beta is (p+1,)
    
    X: feature matrix of shape (n, p)
    beta: coefficient vector including intercept (length p+1)
    """
    # Add column of ones for intercept
    X_aug = np.column_stack([np.ones(X.shape[0]), X])
    return X_aug @ beta

# --- Test the function ---
# Small test dataset
X_test = np.array([[1, 2],
                   [3, 4],
                   [5, 6]])  # shape (3,2)

beta_test = np.array([10, 2, 3]) 
# This means: y = 10 + 2*x1 + 3*x2

y_pred = linear_model(X_test, beta_test)
print("Test predictions:", y_pred)








def augment_X_with_ones(X: np.ndarray) -> np.ndarray:
    """Add intercept column: X' = [1, X]."""
    return np.column_stack([np.ones(X.shape[0]), X])

def mse_cost(X: np.ndarray, beta: np.ndarray, y: np.ndarray) -> float:
    """
    Mean Squared Error:
    J(beta) = (1/n) * || y - X' @ beta ||^2
    X: (n, p), beta: (p+1,), y: (n,)
    """
    X_aug = augment_X_with_ones(X)
    residuals = X_aug @ beta - y
    n = X_aug.shape[0]
    return (residuals @ residuals) / n

def mse_grad(X: np.ndarray, beta: np.ndarray, y: np.ndarray) -> np.ndarray:
    """
    Gradient of MSE w.r.t. beta:
    grad = (2/n) * X'^T * (X' @ beta - y)
    Returns shape (p+1,)
    """
    X_aug = augment_X_with_ones(X)
    residuals = X_aug @ beta - y
    n = X_aug.shape[0]
    return (2.0 / n) * (X_aug.T @ residuals)


# Synthetic small test
np.random.seed(0)
n, p = 5, 3
X_test = np.random.randn(n, p)
beta_test = np.random.randn(p + 1)
y_test = np.random.randn(n)

# Analytical gradient
g_analytical = mse_grad(X_test, beta_test, y_test)

# Finite-difference gradient
eps = 1e-6
g_numeric = np.zeros_like(beta_test)
for j in range(len(beta_test)):
    e = np.zeros_like(beta_test); e[j] = eps
    j_plus  = mse_cost(X_test, beta_test + e, y_test)
    j_minus = mse_cost(X_test, beta_test - e, y_test)
    g_numeric[j] = (j_plus - j_minus) / (2 * eps)

print("Analytical grad:", np.round(g_analytical, 6))
print("Numeric grad   :", np.round(g_numeric, 6))
print("Max |diff|     :", float(np.max(np.abs(g_analytical - g_numeric))))









# --- Reuse from Task 4 ---
def augment_X_with_ones(X: np.ndarray) -> np.ndarray:
    return np.column_stack([np.ones(X.shape[0]), X])

def mse_cost(X: np.ndarray, beta: np.ndarray, y: np.ndarray) -> float:
    X_aug = augment_X_with_ones(X)
    r = X_aug @ beta - y
    return (r @ r) / X.shape[0]

def mse_grad(X: np.ndarray, beta: np.ndarray, y: np.ndarray) -> np.ndarray:
    X_aug = augment_X_with_ones(X)
    r = X_aug @ beta - y
    n = X.shape[0]
    return (2.0 / n) * (X_aug.T @ r)

# --- Utilities for scaling and safe step size ---
class StandardScaler:
    def fit(self, X):
        self.mean_ = X.mean(axis=0)
        self.std_  = X.std(axis=0, ddof=0)
        self.std_[self.std_ == 0.0] = 1.0
        return self
    def transform(self, X):
        return (X - self.mean_) / self.std_
    def fit_transform(self, X):
        return self.fit(X).transform(X)

def safe_eta_from_lipschitz(X: np.ndarray) -> float:
    """
    Compute a safe learning rate via L = (2/n) * lambda_max(X'^T X')
    where X' = [1, X]. Then eta = 1/L.
     """
    X_aug = augment_X_with_ones(X)
    # Compute the largest eigenvalue of X_aug^T X_aug
    # For numerical stability use svd: lambda_max = sigma_max^2
    # where sigma_max is the largest singular value of X_aug.
    smax = np.linalg.svd(X_aug, compute_uv=False)[0]
    n = X.shape[0]
    L = (2.0 / n) * (smax ** 2)
    return 1.0 / (L + 1e-12)

def gradient_descent(
    X: np.ndarray,
    y: np.ndarray,
    eta: float | None = None,
    max_iters: int = 10000,
    tol: float = 1e-8,
    grad_tol: float = 1e-6,
    patience: int = 100,
    normalize: bool = True,
    seed: int = 0,
):
    """
    Perform GD on MSE. Returns beta, history, and metadata.
    - Stopping rules: max_iters, relative cost improvement, gradient norm, patience.
    - If eta is None, use safe Lipschitz-based step size.
    """
    rng = np.random.default_rng(seed)

    # Optionally standardize X (intercept handled by augment function)
    scaler = None
    X_work = X.copy()
    if normalize:
        scaler = StandardScaler()
        X_work = scaler.fit_transform(X_work)

    # Initialize beta: zeros is fine with normalized features
    beta = np.zeros(X_work.shape[1] + 1)

    # Learning rate
    if eta is None:
        eta = safe_eta_from_lipschitz(X_work)

    history = {"cost": [], "grad_norm": [], "eta": eta}
    best_cost = np.inf
    no_improve = 0

    prev_cost = None
    for t in range(1, max_iters + 1):
        g = mse_grad(X_work, beta, y)
        beta -= eta * g

        J = mse_cost(X_work, beta, y)
        gnorm = np.linalg.norm(g)

        history["cost"].append(J)
        history["grad_norm"].append(gnorm)

        # Stopping: gradient small
        if gnorm < grad_tol:
            return beta, history, {"iters": t, "converged": True, "reason": "grad_tol", "eta": eta}

        # Stopping: relative improvement small (after first step)
        if prev_cost is not None:
            rel_drop = (prev_cost - J) / max(prev_cost, 1.0)
            if rel_drop < tol:
                no_improve += 1
            else:
                no_improve = 0

            if no_improve >= patience:
                return beta, history, {"iters": t, "converged": True, "reason": "patience", "eta": eta}

        # Track best (optional)
        if J < best_cost:
            best_cost = J

        prev_cost = J

    return beta, history, {"iters": max_iters, "converged": False, "reason": "max_iters", "eta": eta}



# --- Example run on our CSV dataset ---
df = pd.read_csv("linear_regression_dataset.csv")
X = df.drop(columns=["y"]).to_numpy()
y = df["y"].to_numpy()

beta_gd, hist, info = gradient_descent(
    X, y,
    eta=None,            # auto-compute safe step
    max_iters=20000,
    tol=1e-9,
    grad_tol=1e-6,
    patience=200,
    normalize=True,
)

print("Converged:", info["converged"], "| Reason:", info["reason"], "| Iters:", info["iters"])
print("Step size (eta):", hist["eta"])
print("Final cost:", hist["cost"][-1])
print("Final grad norm:", hist["grad_norm"][-1])
print("Estimated beta (first 6):", np.round(beta_gd[:6], 4))


import matplotlib.pyplot as plt

plt.figure(figsize=(6,4))
plt.plot(hist["cost"])
plt.xlabel("Iteration")
plt.ylabel("MSE cost")
plt.title("Gradient Descent Learning Curve")
plt.grid(True, linewidth=0.3)
plt.show()









# --- Common helpers (reuse from earlier tasks) ---
def augment_X_with_ones(X): return np.column_stack([np.ones(X.shape[0]), X])

class StandardScaler:
    def fit(self, X):
        self.mean_ = X.mean(axis=0)
        self.std_  = X.std(axis=0, ddof=0)
        self.std_[self.std_ == 0.0] = 1.0
        return self
    def transform(self, X): return (X - self.mean_) / self.std_
    def fit_transform(self, X): return self.fit(X).transform(X)

def safe_eta_from_lipschitz(X):
    """Safe step using L = (2/n) * sigma_max(X')^2 (like MSE upper bound).
    We'll scale down for MAE/Huber to be conservative."""
    X_aug = augment_X_with_ones(X)
    smax = np.linalg.svd(X_aug, compute_uv=False)[0]
    n = X.shape[0]
    L = (2.0 / n) * (smax**2)
    return 1.0 / (L + 1e-12)

# --- Losses and (sub)gradients ---
def cost_mae(X, beta, y):
    X_aug = augment_X_with_ones(X)
    r = X_aug @ beta - y
    return np.mean(np.abs(r))

def grad_mae(X, beta, y):
    X_aug = augment_X_with_ones(X)
    r = X_aug @ beta - y
    s = np.sign(r)            # subgradient; sign(0)=0
    return (X_aug.T @ s) / X.shape[0]

def cost_huber(X, beta, y, delta=1.0):
    X_aug = augment_X_with_ones(X)
    r = X_aug @ beta - y
    a = np.abs(r)
    quad = 0.5 * (r**2)
    lin  = delta * (a - 0.5 * delta)
    return np.mean(np.where(a <= delta, quad, lin))

def grad_huber(X, beta, y, delta=1.0):
    X_aug = augment_X_with_ones(X)
    r = X_aug @ beta - y
    a = np.abs(r)
    psi = np.where(a <= delta, r, delta * np.sign(r))
    return (X_aug.T @ psi) / X.shape[0]

# --- Generic GD driver (works for MAE/Huber, also MSE if you pass its cost/grad) ---
def gradient_descent_generic(
    X, y, cost_fn, grad_fn, *,
    eta=None, max_iters=20000, tol=1e-8, grad_tol=1e-6, patience=200,
    normalize=True, seed=0, grad_kwargs=None, cost_kwargs=None, eta_scale=1.0
):
    rng = np.random.default_rng(seed)
    grad_kwargs = grad_kwargs or {}
    cost_kwargs = cost_kwargs or {}

    # Optional standardization (recommended for stability)
    scaler = None
    X_work = X.copy()
    if normalize:
        scaler = StandardScaler()
        X_work = scaler.fit_transform(X_work)

    # Init
    beta = np.zeros(X_work.shape[1] + 1)

    # Learning rate
    if eta is None:
        eta = safe_eta_from_lipschitz(X_work) * eta_scale  # often need smaller steps vs MSE

    hist = {"cost": [], "grad_norm": [], "eta": eta}
    best_cost = np.inf; no_improve = 0; prev_cost = None
    t0 = perf_counter()
    for t in range(1, max_iters + 1):
        g = grad_fn(X_work, beta, y, **grad_kwargs)
        beta -= eta * g
        J = cost_fn(X_work, beta, y, **cost_kwargs)
        gnorm = np.linalg.norm(g)
        hist["cost"].append(J); hist["grad_norm"].append(gnorm)

        # Stopping: gradient small (works well for Huber; for MAE this is a subgradient)
        if gnorm < grad_tol:
            dt = perf_counter() - t0
            return beta, hist, {"iters": t, "converged": True, "reason": "grad_tol", "eta": eta, "time_s": dt}

        # Stopping: relative cost improvement with patience
        if prev_cost is not None:
            rel_drop = (prev_cost - J) / max(prev_cost, 1.0)
            if rel_drop < tol: no_improve += 1
            else: no_improve = 0
            if no_improve >= patience:
                dt = perf_counter() - t0
                return beta, hist, {"iters": t, "converged": True, "reason": "patience",
                                    "eta": eta, "time_s": dt}
        prev_cost = J

    dt = perf_counter() - t0
    return beta, hist, {"iters": max_iters, "converged": False, "reason": "max_iters", 
                        "eta": eta, "time_s": dt}


# Load your data
df = pd.read_csv("linear_regression_dataset.csv")
X = df.drop(columns=["y"]).to_numpy()
y = df["y"].to_numpy()

# (Optional) Inject outliers to test robustness:
# out_idx = np.random.default_rng(123).choice(len(y), size=5, replace=False)
# y[out_idx] += 800  # large positive outliers

# MAE: subgradient descent — smaller eta, looser grad_tol typically helps
beta_mae, hist_mae, info_mae = gradient_descent_generic(
    X, y, cost_mae, grad_mae,
    eta=None, eta_scale=0.25,     # shrink LR vs MSE heuristic
    max_iters=50000, tol=1e-9, grad_tol=5e-5, patience=1000,
    normalize=True
)

# Huber: choose delta relative to noise scale; try delta ≈ 1–2× RMSE(MSE) or use a robust scale
delta = 5.0
beta_hub, hist_hub, info_hub = gradient_descent_generic(
    X, y,
    lambda X_, b, y_: cost_huber(X_, b, y_, delta=delta),
    lambda X_, b, y_: grad_huber(X_, b, y_, delta=delta),
    eta=None, eta_scale=0.5,      # Huber tolerates larger steps than MAE
    max_iters=20000, tol=1e-9, grad_tol=1e-6, patience=500,
    normalize=True
)

print("MAE   -> Converged:", info_mae["converged"], "| Reason:", info_mae["reason"],
      "| Iters:", info_mae["iters"], "| Time (s):", round(info_mae["time_s"], 4),
      "| Final cost:", round(hist_mae["cost"][-1], 6))

print("Huber -> Converged:", info_hub["converged"], "| Reason:", info_hub["reason"],
      "| Iters:", info_hub["iters"], "| Time (s):", round(info_hub["time_s"], 4),
      "| Final cost:", round(hist_hub["cost"][-1], 6), "| delta:", delta)

























